---
widget: blank
headless: true

# ... Put Your Section Options Here (title etc.) ...
title: News
subtitle:
weight: 10  # section position on page
design:
  # Choose how many columns the section has. Valid values: 1 or 2.
  columns: '1'
---

- **April 2023:** I gave an invited talk at the [SPP 2353 Summer School](https://www.itm.uni-stuttgart.de/en/spp_2353/) focusing on "Neural Network Training through the Lens of Benchmarking and Debugging".
- **December 2022:** I organized the ["**Has it Trained Yet? A Workshop for Algorithmic Efficiency in Practical Neural Network Training**"](https://hity-workshop.github.io/NeurIPS2022/) workshop at [NeurIPS 2022](https://nips.cc/Conferences/2022). We also present our workshop paper ["Late-Phase Second-Order Training"](https://neurips.cc/Conferences/2022/ScheduleMultitrack?event=56640) at NeurIPS 2022.
- **July 2022:** I succesfully defended my Ph.D. thesis with the title [**Understanding Deep Learning Optimization via Benchmarking and Debugging**](https://publikationen.uni-tuebingen.de/xmlui/handle/10900/131710)!
  </br>*I will continue to work as a postdoctoral researcher at the University of TÃ¼bingen.*
- **April 2022:** I gave an invited talk at the [ML Evaluation Standards Workshop](https://ml-eval.github.io/) at [ICLR 2022](https://iclr.cc/Conferences/2022).
  </br> *During the talk and the panel discussion, we examined the role of reproducibility and rigor in machine learning.*
- **September 2021:** Our paper "**Cockpit: A Practical Debugging Tool for the Training of Deep Neural Networks**" has been accepted at [NeurIPS 2021](https://nips.cc/Conferences/2021/).
  </br>*In this work, we present a new kind of debugger, specifically designed for training deep nets.*
- **May 2021:** Our work "**Descending through a Crowded Valley - Benchmarking Deep Learning Optimziers**" has been accepted at [ICML 2021](https://icml.cc/Conferences/2021).
  </br>*In it, we present an extensive comparison of fifteen popular deep learning optimizers.*
- **April, 2021:** I have been elected as a co-chair for the [MLCommons](https://mlcommons.org/en/) working group on [Algorithmic Efficiency](https://mlcommons.org/en/groups/research-algorithms/) together with [George Dahl](https://www.cs.toronto.edu/~gdahl/).
  </br>*The working group will develop a set of rigorous and relevant benchmarks to measure training speedups to neural network training due to algorithmic improvements, focusing on new training algorithms and models.*
- **September & Oktober 2020:** I have been distinguished as a top reviewer for ICML 2020 and received a Top 10% Reviewer award for NeurIPS 2020.
- **September 2019:** I presented our work on "DeepOBS" during an invited talk at the [Machine Learning Lab](http://aad.informatik.uni-freiburg.de/people/hutter/) at the University of Freiburg.
- **May 2019:** Our paper "**DeepOBS: A Deep Learning Optimizer Benchmark Suite**" has been accepted at [ICLR 2019](https://iclr.cc/Conferences/2019).
  </br>*In the paper, we present a benchmark suite for deep learning optimization methods. I will be at the conference from 06th through 09th May in New Orleans, USA.*
- **July & September 2018:** I attended the [Microsoft Research AI Summer School 2018](https://www.microsoft.com/en-us/research/event/ai-summer-school-2018/ "Microsoft Research AI Summer School") in Cambridge and the [Gaussian Process Summer School 2018](http://gpss.cc/gpss18/ "GPSS18") in Sheffield.
